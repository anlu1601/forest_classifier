{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, ZeroPadding2D\n",
    "from keras.layers import Flatten, Dense, Dropout, BatchNormalization\n",
    "from keras.layers import Input, concatenate\n",
    "from keras.models import Model, load_model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import plot_model,np_utils\n",
    "from keras import regularizers\n",
    "import keras.metrics as metric\n",
    "import os\n",
    "\n",
    "# Global Constants\n",
    "NB_CLASS = 2\n",
    "LEARNING_RATE = 0.01\n",
    "MOMENTUM = 0.9\n",
    "ALPHA = 0.0001\n",
    "BETA = 0.75\n",
    "GAMMA = 0.1\n",
    "DROPOUT = 0.4\n",
    "WEIGHT_DECAY = 0.0005\n",
    "LRN2D_NORM = True\n",
    "DATA_FORMAT = 'channels_last' # Theano:'channels_first' Tensorflow:'channels_last'\n",
    "USE_BN = True\n",
    "IM_WIDTH = 224\n",
    "IM_HEIGHT = 224\n",
    "EPOCH = 10 #50\n",
    "\n",
    "train_root = 'after_divide_dataset/train/'\n",
    "validation_root = 'after_divide_dataset/val/'\n",
    "test_root = 'after_divide_dataset/test/'\n",
    "IM_WIDTH = 224\n",
    "IM_HEIGHT = 224\n",
    "batch_size = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 159 images belonging to 2 classes.\n",
      "Found 19 images belonging to 2 classes.\n",
      "Found 22 images belonging to 2 classes.\n",
      "22\n",
      "train {'birch': 0, 'spruce': 1}\n",
      "valid {'birch': 0, 'spruce': 1}\n",
      "test {'birch': 0, 'spruce': 1}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# train data\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range = 30,\n",
    "    width_shift_range = 0.2,\n",
    "    height_shift_range = 0.2,\n",
    "    shear_range = 0.2,\n",
    "    zoom_range = 0.2,\n",
    "    horizontal_flip = True,\n",
    "    featurewise_center = False\n",
    "    # featurewise_center: 3 channels of the original image value-the mean value of the 3 channels of the original image value\n",
    ")\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_root,\n",
    "    target_size=(IM_WIDTH, IM_HEIGHT),\n",
    "    batch_size = batch_size,\n",
    ")\n",
    "\n",
    "# valid data\n",
    "valid_datagen = ImageDataGenerator(\n",
    "    rotation_range = 30,\n",
    "    width_shift_range = 0.2,\n",
    "    height_shift_range = 0.2,\n",
    "    shear_range = 0.2,\n",
    "    zoom_range = 0.2,\n",
    "    horizontal_flip = True,\n",
    "    featurewise_center = False\n",
    ")\n",
    "valid_generator = train_datagen.flow_from_directory(\n",
    "    validation_root,\n",
    "    target_size = (IM_WIDTH, IM_HEIGHT),\n",
    "    batch_size = batch_size,\n",
    ")\n",
    "\n",
    "#test data\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rotation_range = 30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range = 0.2,\n",
    "    zoom_range = 0.2,\n",
    "    horizontal_flip=True,\n",
    "    featurewise_center=False\n",
    ")\n",
    "test_generator = train_datagen.flow_from_directory(\n",
    "    test_root,\n",
    "    target_size=(IM_WIDTH, IM_HEIGHT),\n",
    "    batch_size = batch_size,\n",
    ")\n",
    "# Function: flow_from_directory(classes), classes is optional sub-category list (for example ['dogs','cats']). \n",
    "# Each subdirectory will be treated as a different class (the class name will be lexicographically\n",
    "# mapped to the index of the label).The dictionary containing the mapping from class name to class\n",
    "# index can be obtained through the class_indices attribute.\n",
    "print(test_generator.n)\n",
    "print(\"train\",train_generator.class_indices)\n",
    "print(\"valid\",valid_generator.class_indices)\n",
    "print(\"test\",test_generator.class_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#normalization\n",
    "def conv2D_lrn2d(x, filters, kernel_size, strides=(1,1), padding='same', \n",
    "                 data_format=DATA_FORMAT, dilation_rate=(1,1) ,activation='relu',\n",
    "                 use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zero',\n",
    "                 kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None,\n",
    "                 kernel_constraint=None, bias_constraint=None, lrn2d_norm=LRN2D_NORM, \n",
    "                 weight_decay=WEIGHT_DECAY):\n",
    "    #l2 normalization\n",
    "    if weight_decay:\n",
    "        kernel_regularizer = regularizers.l2(weight_decay)\n",
    "        bias_regularizer = regularizers.l2(weight_decay)\n",
    "    else:\n",
    "        kernel_regularizer = None\n",
    "        bias_regularizer = None\n",
    "\n",
    "        X = Conv2D(filters=filters, kernel_size=kernel_size, strides=strides, padding=padding, \n",
    "                 data_format=data_format, dilation_rate = dilation_rate, activation=activation,\n",
    "                 use_bias=use_bias, kernel_initializer=kernel_initializer, \n",
    "                 bias_initializer=bias_initializer, kernel_regularizer=kernel_regularizer,\n",
    "                 bias_regularizer=bias_regularizer,activity_regularizer=activity_regularizer,\n",
    "                 kernel_constraint=kernel_constraint, bias_constraint=bias_constraint,\n",
    "                 )(x)\n",
    "  \n",
    "    if lrn2d_norm:\n",
    "        #batch normalization\n",
    "        x=BatchNormalization()(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inception_module(x,params,concat_axis,padding='same',data_format=DATA_FORMAT,\n",
    "                     dilation_rate=(1,1),activation='relu',use_bias=True,\n",
    "                     kernel_initializer='glorot_uniform',bias_initializer='zeros',\n",
    "                     kernel_regularizer=None,bias_regularizer=None,activity_regularizer=None,\n",
    "                     kernel_constraint=None,bias_constraint=None,lrn2d_norm=LRN2D_NORM,weight_decay=None):\n",
    "    (branch1,branch2,branch3,branch4)=params\n",
    "    if weight_decay:\n",
    "        kernel_regularizer=regularizers.l2(weight_decay)\n",
    "        bias_regularizer=regularizers.l2(weight_decay)\n",
    "    else:\n",
    "        kernel_regularizer=None\n",
    "        bias_regularizer=None\n",
    "    #1x1\n",
    "    pathway1=Conv2D(filters=branch1[0],kernel_size=(1,1),strides=1,padding=padding,\n",
    "                    data_format=data_format,dilation_rate=dilation_rate,activation=activation,\n",
    "                    use_bias=use_bias,kernel_initializer=kernel_initializer,bias_initializer=bias_initializer,\n",
    "                    kernel_regularizer=kernel_regularizer,bias_regularizer=bias_regularizer,\n",
    "                    activity_regularizer=activity_regularizer,kernel_constraint=kernel_constraint,\n",
    "                    bias_constraint=bias_constraint)(x)\n",
    "\n",
    "    #1x1->3x3\n",
    "    pathway2=Conv2D(filters=branch2[0],kernel_size=(1,1),strides=1,padding=padding,\n",
    "                    data_format=data_format,dilation_rate=dilation_rate,activation=activation,\n",
    "                    use_bias=use_bias,kernel_initializer=kernel_initializer,bias_initializer=bias_initializer,\n",
    "                    kernel_regularizer=kernel_regularizer,bias_regularizer=bias_regularizer,\n",
    "                    activity_regularizer=activity_regularizer,kernel_constraint=kernel_constraint,\n",
    "                    bias_constraint=bias_constraint)(x)\n",
    "    pathway2=Conv2D(filters=branch2[1],kernel_size=(3,3),strides=1,padding=padding,\n",
    "                    data_format=data_format,dilation_rate=dilation_rate,activation=activation,\n",
    "                    use_bias=use_bias,kernel_initializer=kernel_initializer,bias_initializer=bias_initializer,\n",
    "                    kernel_regularizer=kernel_regularizer,bias_regularizer=bias_regularizer,\n",
    "                    activity_regularizer=activity_regularizer,kernel_constraint=kernel_constraint,\n",
    "                    bias_constraint=bias_constraint)(pathway2)\n",
    "\n",
    "    #1x1->5x5\n",
    "    pathway3=Conv2D(filters=branch3[0],kernel_size=(1,1),strides=1,padding=padding,\n",
    "                    data_format=data_format,dilation_rate=dilation_rate,activation=activation,\n",
    "                    use_bias=use_bias,kernel_initializer=kernel_initializer,bias_initializer=bias_initializer,\n",
    "                    kernel_regularizer=kernel_regularizer,bias_regularizer=bias_regularizer,\n",
    "                    activity_regularizer=activity_regularizer,kernel_constraint=kernel_constraint,\n",
    "                    bias_constraint=bias_constraint)(x)\n",
    "    pathway3=Conv2D(filters=branch3[1],kernel_size=(5,5),strides=1,padding=padding,\n",
    "                    data_format=data_format,dilation_rate=dilation_rate,activation=activation,\n",
    "                    use_bias=use_bias,kernel_initializer=kernel_initializer,bias_initializer=bias_initializer,\n",
    "                    kernel_regularizer=kernel_regularizer,bias_regularizer=bias_regularizer,\n",
    "                    activity_regularizer=activity_regularizer,kernel_constraint=kernel_constraint,\n",
    "                    bias_constraint=bias_constraint)(pathway3)\n",
    "\n",
    "    #3x3->1x1\n",
    "    pathway4=MaxPooling2D(pool_size=(3,3),strides=1,padding=padding,data_format=DATA_FORMAT)(x)\n",
    "    pathway4=Conv2D(filters=branch4[0],kernel_size=(1,1),strides=1,padding=padding,\n",
    "                    data_format=data_format,dilation_rate=dilation_rate,activation=activation,\n",
    "                    use_bias=use_bias,kernel_initializer=kernel_initializer,bias_initializer=bias_initializer,\n",
    "                    kernel_regularizer=kernel_regularizer,bias_regularizer=bias_regularizer,\n",
    "                    activity_regularizer=activity_regularizer,kernel_constraint=kernel_constraint,\n",
    "                    bias_constraint=bias_constraint)(pathway4)\n",
    "\n",
    "    return concatenate([pathway1,pathway2,pathway3,pathway4],axis=concat_axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    #Data format:tensorflow,channels_last;theano,channels_last\n",
    "    if DATA_FORMAT=='channels_first':\n",
    "        INP_SHAPE=(3,224,224)\n",
    "        img_input=Input(shape=INP_SHAPE)\n",
    "        CONCAT_AXIS=1\n",
    "    elif DATA_FORMAT=='channels_last':\n",
    "        INP_SHAPE=(224,224,3)\n",
    "        img_input=Input(shape=INP_SHAPE)\n",
    "        CONCAT_AXIS=3\n",
    "    else:\n",
    "        raise Exception('Invalid Dim Ordering')\n",
    "\n",
    "    x=conv2D_lrn2d(img_input,64,(7,7),2,padding='same',lrn2d_norm=False)\n",
    "    x=MaxPooling2D(pool_size=(3,3),strides=2,padding='same',data_format=DATA_FORMAT)(x)\n",
    "    x=BatchNormalization()(x)\n",
    "\n",
    "    x=conv2D_lrn2d(x,64,(1,1),1,padding='same',lrn2d_norm=False)\n",
    "\n",
    "    x=conv2D_lrn2d(x,192,(3,3),1,padding='same',lrn2d_norm=True)\n",
    "    x=MaxPooling2D(pool_size=(3,3),strides=2,padding='same',data_format=DATA_FORMAT)(x)\n",
    "\n",
    "    x=inception_module(x,params=[(64,),(96,128),(16,32),(32,)],concat_axis=CONCAT_AXIS) #3a\n",
    "    x=inception_module(x,params=[(128,),(128,192),(32,96),(64,)],concat_axis=CONCAT_AXIS) #3b\n",
    "    x=MaxPooling2D(pool_size=(3,3),strides=2,padding='same',data_format=DATA_FORMAT)(x)\n",
    "\n",
    "    x=inception_module(x,params=[(192,),(96,208),(16,48),(64,)],concat_axis=CONCAT_AXIS) #4a\n",
    "    x=inception_module(x,params=[(160,),(112,224),(24,64),(64,)],concat_axis=CONCAT_AXIS) #4b\n",
    "    x=inception_module(x,params=[(128,),(128,256),(24,64),(64,)],concat_axis=CONCAT_AXIS) #4c\n",
    "    x=inception_module(x,params=[(112,),(144,288),(32,64),(64,)],concat_axis=CONCAT_AXIS) #4d\n",
    "    x=inception_module(x,params=[(256,),(160,320),(32,128),(128,)],concat_axis=CONCAT_AXIS) #4e\n",
    "    x=MaxPooling2D(pool_size=(3,3),strides=2,padding='same',data_format=DATA_FORMAT)(x)\n",
    "\n",
    "    x=inception_module(x,params=[(256,),(160,320),(32,128),(128,)],concat_axis=CONCAT_AXIS) #5a\n",
    "    x=inception_module(x,params=[(384,),(192,384),(48,128),(128,)],concat_axis=CONCAT_AXIS) #5b\n",
    "    x=AveragePooling2D(pool_size=(7,7),strides=1,padding='valid',data_format=DATA_FORMAT)(x)\n",
    "\n",
    "    x=Flatten()(x)\n",
    "    x=Dropout(DROPOUT)(x)\n",
    "    x=Dense(NB_CLASS,activation='linear')(x)\n",
    "    x=Dense(NB_CLASS,activation='softmax')(x)\n",
    "    # x=Dense(output_dim=NB_CLASS,activation='linear')(x)\n",
    "    # x=Dense(output_dim=NB_CLASS,activation='softmax')(x)\n",
    "\n",
    "    return x,img_input,CONCAT_AXIS,INP_SHAPE,DATA_FORMAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_print():\n",
    "    # Create the Model\n",
    "    x,img_input,CONCAT_AXIS,INP_SHAPE,DATA_FORMAT=create_model()\n",
    "\n",
    "    # Create a Keras Model\n",
    "    # model=Model(input=img_input,output=[x])\n",
    "    model=Model(img_input,[x])\n",
    "    model.summary()\n",
    "\n",
    "    # Save a PNG of the Model Build\n",
    "    plot_model(model,to_file='GoogLeNet.png')\n",
    "\n",
    "    model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['acc',metric.top_k_categorical_accuracy])\n",
    "    print('Model Compiled')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    if os.path.exists('inception_1.h5'):\n",
    "        model=load_model('inception_1.h5')\n",
    "    else:\n",
    "        model=check_print()\n",
    "\n",
    "    \n",
    "    model.fit(train_generator,vali dation_data=valid_generator,epochs=EPOCH,\n",
    "                        steps_per_epoch=train_generator.n/batch_size\n",
    "                        ,validation_steps=valid_generator.n/batch_size)\n",
    "#     model.fit(train_generator,epochs=EPOCH,\n",
    "#                     steps_per_epoch=train_generator.n/batch_size\n",
    "#                     )\n",
    "    model.save('inception_1.h5')\n",
    "    model.METRICS=['acc',metric.top_k_categorical_accuracy]\n",
    "    # loss,acc,top_acc=model.evaluate_generator(test_generator,steps=test_generator.n/batch_size)\n",
    "    loss,acc,top_acc=model.evaluate(test_generator,steps=test_generator.n/batch_size)\n",
    "    print('Test result:loss:%f,acc:%f,top_acc:%f'%(loss,acc,top_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
